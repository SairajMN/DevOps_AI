# DevOps Log Intelligence & Auto-Triage System

A local-first, deterministic-first, AI-assisted log intelligence system for automated error detection, classification, and fix suggestion. Now with **OpenRouter AI Integration** for advanced LLM-powered analysis.

## ğŸ¯ Overview

This system provides intelligent log analysis capabilities:

- **Monitors** project logs in real-time
- **Detects** and **classifies** failures using pattern matching
- **Analyzes** project configuration for context
- **Proposes** deterministic fixes
- **Generates** structured patch suggestions
- **Stores** historical resolution memory
- **AI-Powered Analysis** via OpenRouter (DeepSeek, Llama, Mistral, Qwen, Gemini)
- **Requests** manual approval before any action

## ğŸ—ï¸ Architecture

### Core Pipeline
```
Project Folder
      â†“
Log Watcher Engine
      â†“
Structured Log Parser
      â†“
Error Classifier (Rule-first)
      â†“
Context Analyzer
      â†“
Deterministic Fix Engine
      â†“
Patch Proposal Generator
      â†“
Incident Memory Store
      â†“
Report Builder
```

### AI Integration Architecture
```
User Request
      â†“
API (Express Server)
      â†“
Accomplish Agent (Orchestrator)
      â†“
Tool Layer
   â”œâ”€â”€ Log Parser
   â”œâ”€â”€ Shell Executor
   â”œâ”€â”€ Git Tool
   â”œâ”€â”€ File System
      â†“
OpenRouter LLM Layer
   â”œâ”€â”€ DeepSeek R1 (reasoning)
   â”œâ”€â”€ DeepSeek Chat (code)
   â”œâ”€â”€ Qwen 2.5 (fallback)
   â”œâ”€â”€ Llama 3.1 8B (documentation)
   â”œâ”€â”€ Mistral 7B (quick tasks)
   â”œâ”€â”€ Gemini Flash 1.5 (general)
      â†“
Response + Suggested Fix
```

## ğŸ“ Project Structure

```
devops-intelligence/
â”œâ”€â”€ src/                    # TypeScript AI Integration
â”‚   â”œâ”€â”€ ai/                 # AI/LLM Layer
â”‚   â”‚   â”œâ”€â”€ models.ts       # Model registry
â”‚   â”‚   â”œâ”€â”€ modelRouter.ts  # Smart model selection
â”‚   â”‚   â””â”€â”€ openrouterClient.ts  # OpenRouter API client
â”‚   â”œâ”€â”€ agent/              # Agent orchestration
â”‚   â”‚   â”œâ”€â”€ accomplishAgent.ts   # Main agent
â”‚   â”‚   â”œâ”€â”€ taskOrchestrator.ts  # Task management
â”‚   â”‚   â””â”€â”€ prompts.ts      # Production-grade prompts
â”‚   â”œâ”€â”€ routes/             # API routes
â”‚   â”‚   â””â”€â”€ analyze.ts      # Analysis endpoints
â”‚   â””â”€â”€ index.ts            # Express server entry
â”œâ”€â”€ analyzer/               # Codebase analysis
â”œâ”€â”€ classifier/             # Error classification
â”œâ”€â”€ fix_engine/             # Fix generation engine
â”œâ”€â”€ memory/                 # Incident memory store
â”œâ”€â”€ parser/                 # Log parsing modules
â”œâ”€â”€ patch/                  # Patch generation
â”œâ”€â”€ reports/                # Report generation
â”œâ”€â”€ watcher/                # Log monitoring
â”œâ”€â”€ logs/                   # Log files to analyze
â”œâ”€â”€ storage/                # Persistent storage
â”œâ”€â”€ test/                   # Test files
â”œâ”€â”€ main.py                 # Python main entry
â”œâ”€â”€ cli.py                  # CLI interface
â”œâ”€â”€ config.py               # Configuration module
â”œâ”€â”€ package.json            # Node.js dependencies
â”œâ”€â”€ tsconfig.json           # TypeScript config
â””â”€â”€ requirements.txt        # Python dependencies
```

## ğŸš€ Quick Start

### Prerequisites
- Node.js 18+
- Python 3.8+
- OpenRouter API Key (get one at [openrouter.ai](https://openrouter.ai))

### Installation

```bash
# Clone the repository
git clone https://github.com/SairajMN/DevOps_AI.git
cd DevOps_AI

# Install Node.js dependencies
npm install

# Create virtual environment for Python
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install Python dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env and add your OPENROUTER_API_KEY
```

### Running the AI Server

```bash
# Development mode
npm run dev

# Build and run production
npm run build
npm start
```

### Using the CLI

```bash
# Show system status
python cli.py status

# Analyze a log file
python cli.py analyze --file logs/sample_errors.log

# Start monitoring mode
python cli.py monitor --paths /var/log/app.log

# View incident history
python cli.py history --limit 20

# Generate a report
python cli.py report --type summary
```

## ğŸ”Œ API Endpoints

### Server runs on `http://localhost:3000` by default

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | API info |
| `/api/health` | GET | Health check |
| `/api/models` | GET | List available AI models |
| `/api/models/:modelId` | GET | Get model details |
| `/api/analyze` | POST | Full log analysis |
| `/api/analyze/quick` | POST | Quick analysis |
| `/api/analyze/multi` | POST | Multi-step analysis |
| `/api/analyze/batch` | POST | Batch analysis |
| `/api/fix` | POST | Code fix generation |
| `/api/tasks` | POST | Create task |
| `/api/tasks/:taskId` | GET | Get task status |
| `/api/tasks/:taskId/execute` | POST | Execute task |
| `/api/queue` | GET | Get queue status |

### Example API Calls

```bash
# Analyze a log
curl -X POST http://localhost:3000/api/analyze \
  -H "Content-Type: application/json" \
  -d '{"log": "ERROR: Connection timeout at database.py:45"}'

# Quick analysis
curl -X POST http://localhost:3000/api/analyze/quick \
  -H "Content-Type: application/json" \
  -d '{"log": "TypeError: Cannot read property of undefined"}'

# Generate code fix
curl -X POST http://localhost:3000/api/fix \
  -H "Content-Type: application/json" \
  -d '{
    "code": "def add(a, b):\n    return a + b",
    "errorMessage": "TypeError: unsupported operand type",
    "language": "python"
  }'

# Get available models
curl http://localhost:3000/api/models
```

## ğŸ¤– AI Models

### Available Models via OpenRouter

| Model ID | Name | Best For |
|----------|------|----------|
| `deepseek-r1` | DeepSeek R1 | Reasoning, debugging, log analysis |
| `deepseek-v3` | DeepSeek V3 | Code generation, refactoring |
| `llama-70b` | Llama 3.1 8B | Documentation, general tasks |
| `mixtral` | Mistral 7B | Quick fallback, Python/JS |
| `qwen` | Qwen 2.5 7B | Coding, reasoning |
| `gemini-flash` | Gemini Flash 1.5 | Fast general tasks |

### Smart Model Selection

The system automatically selects the best model based on task type:

```typescript
// Automatic selection
Task Type          â†’ Model
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log-analysis       â†’ DeepSeek R1
debugging          â†’ DeepSeek R1
code-generation    â†’ DeepSeek V3
documentation      â†’ Llama 3.1 8B
quick-fallback     â†’ Mistral 7B
```

### Fallback Chain

If the primary model fails, the system automatically falls back:

```
Primary â†’ Fallback 1 â†’ Fallback 2
```

## ğŸ“‹ CLI Commands

### `monitor` - Start Log Monitoring

```bash
python cli.py monitor --paths /path/to/log1.log /path/to/log2.log --project .
```

### `analyze` - Analyze Log File

```bash
python cli.py analyze --file error.log --format json
```

### `report` - Generate Reports

```bash
python cli.py report --type trend
python cli.py report --incident incident_20260221_abc123
```

### `history` - View Incident History

```bash
python cli.py history --limit 20 --type database_errors
```

### `patch` - Patch Management

```bash
python cli.py patch --list
python cli.py patch --view patch_20260221_1234
python cli.py patch --approve patch_20260221_1234
```

## ğŸ”§ Configuration

### Environment Variables (.env)

```bash
# Required
OPENROUTER_API_KEY=your-key-here

# Server
PORT=3000
HOST=0.0.0.0
NODE_ENV=development

# Optional
LOG_LEVEL=info
DEFAULT_MODEL=deepseek/deepseek-r1
```

### Python Configuration (config.py)

- `log_paths`: Paths to monitor for log files
- `log_patterns`: Patterns to identify error logs
- `poll_interval`: Monitoring poll interval
- `confidence_threshold`: Minimum confidence threshold

## ğŸ¯ Supported Environments

- Node.js
- React / Next.js
- Python (FastAPI, Django)
- Docker
- GitHub Actions logs (exported)
- Vercel build logs (exported)
- Basic Kubernetes logs

## ğŸ“Š Error Categories

| Category | Examples |
|----------|----------|
| `database_errors` | Connection timeout, deadlock, constraint violation |
| `network_errors` | Connection refused, timeout, SSL errors |
| `application_errors` | Null pointer, out of memory, permission denied |
| `authentication_errors` | Invalid credentials, token expired, unauthorized |
| `system_errors` | Disk full, CPU overload, service unavailable |

## ğŸ”’ Security Model

- âœ… Fully local execution
- âœ… No auto patch application
- âœ… No arbitrary command execution
- âœ… Suggested commands are sandboxed text only
- âœ… Explicit approval required for all actions
- âœ… API key stored in environment variables

## ğŸ“ˆ Confidence Scoring

```
confidence = 
  (pattern_weight * 0.4) +
  (context_validation * 0.3) +
  (memory_success_rate * 0.3)
```

Thresholds:
- `â‰¥ 0.85` â†’ High confidence (auto-suggest)
- `0.6 - 0.85` â†’ Review suggested
- `< 0.6` â†’ AI fallback (if enabled)

## ğŸ§ª Testing

```bash
# Run TypeScript integration tests
npx ts-node test/test-integration.ts

# Run with sample log file
python cli.py analyze --file logs/sample_errors.log --format text

# Check system status
python cli.py status

# Type check
npm run typecheck
```

## ğŸ“ Response Format

### Log Analysis Response

```json
{
  "success": true,
  "analysis": {
    "error_type": "ModuleNotFoundError",
    "error_category": "dependency",
    "root_cause": "Missing psycopg2 module in Python environment",
    "confidence": 92,
    "suggested_fix": "Install psycopg2-binary package",
    "step_by_step_fix": [
      "Run: pip install psycopg2-binary",
      "Or add to requirements.txt",
      "Restart the application"
    ],
    "is_environment_issue": false,
    "is_dependency_issue": true,
    "is_code_issue": false,
    "affected_files": ["database.py"],
    "severity": "high"
  },
  "metadata": {
    "model": "deepseek/deepseek-r1",
    "duration": 2340,
    "attempts": 1
  }
}
```

## ğŸ”„ Integration with Existing Python Pipeline

The TypeScript AI layer integrates seamlessly with the existing Python pipeline:

1. **Python Pipeline** handles local log watching, parsing, and deterministic fixes
2. **TypeScript AI Layer** provides advanced LLM-powered analysis via OpenRouter
3. Both can run independently or together

## ğŸ› ï¸ Extending the System

### Adding Custom AI Models

Edit `src/ai/models.ts`:

```typescript
{
    id: "custom-model",
    name: "Custom Model",
    model: "provider/model-name",
    description: "Description",
    strengths: ["strength1", "strength2"],
    maxTokens: 4096,
    taskTypes: ["task-type-1", "task-type-2"]
}
```

### Adding Custom Patterns

Edit `parser/patterns.py`:

```python
ParsePattern(
    name="custom_format",
    pattern=re.compile(r'your-regex-here'),
    fields=["timestamp", "level", "message"],
    description="Custom log format",
    priority=10
)
```

## ğŸ“š Tech Stack

### TypeScript/Node.js
- Express.js - API server
- Axios - HTTP client
- Zod - Schema validation
- TypeScript - Type safety

### Python
- asyncio - Async operations
- watchdog - File monitoring
- Jinja2 - Templating
- markdown - Report generation

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests
5. Submit a pull request

## ğŸ“„ License

MIT License - See LICENSE file for details.

## ğŸ™ Acknowledgments

Built with:
- **AI Models**: DeepSeek, Llama, Mistral, Qwen, Gemini (via OpenRouter)
- **Runtime**: Node.js + Python
- **Frameworks**: Express, asyncio
- **Monitoring**: watchdog